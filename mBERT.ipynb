{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aylin-Rodriguez/Aylin-Rodriguez/blob/main/mBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11b1GI4I4xn0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets scikit-learn openpyxl pandas\n",
        "!pip install tensorflow-cpu\n",
        "!pip install \"numpy<2.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j89SVb_Ho24"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "# Configurar dispositivo de cómputo\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Lee el archivo\n",
        "file_path = '/workspace/dataset_7labels.xlsx'  # Cambia por el nombre correcto del archivo\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "# Cargar las hojas Training y References\n",
        "df_training = pd.read_excel(xls, sheet_name=\"Training\")\n",
        "df_references = pd.read_excel(xls, sheet_name=\"References\")\n",
        "\n",
        "# Combinar ambas hojas en un único dataframe\n",
        "df_combined = pd.concat([df_training, df_references], ignore_index=True)\n",
        "\n",
        "# Dividir el dataset en entrenamiento (70%) y validación (30%)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df_combined, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKQazZ4nH2ru"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Cargar el tokenizer de BERT Multilingüe\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "# Preprocesar las preguntas y respuestas\n",
        "def preprocess_data(df):\n",
        "    # Convierte las columnas de 'Pregunta' y 'Respuesta' a strings y maneja NaNs\n",
        "    df['Pregunta'] = df['Pregunta'].astype(str).fillna('')\n",
        "    df['Respuesta'] = df['Respuesta'].astype(str).fillna('')\n",
        "    inputs = tokenizer(\n",
        "        df['Pregunta'].tolist(),\n",
        "        df['Respuesta'].tolist(),\n",
        "        max_length=128,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "\n",
        "# Preprocesar los datos de entrenamiento y validación\n",
        "train_inputs = preprocess_data(train_df)\n",
        "val_inputs = preprocess_data(val_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ann5TIk5IC9Q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Crear un Dataset personalizado para BERT\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Convertir etiquetas a formato numérico\n",
        "train_labels = train_df['Puntos'].values\n",
        "val_labels = val_df['Puntos'].values\n",
        "\n",
        "# Calcular el mínimo y máximo de las etiquetas\n",
        "label_min = train_labels.min()\n",
        "label_max = train_labels.max()\n",
        "\n",
        "# Normalizar etiquetas de entrenamiento y validación\n",
        "train_labels_normalized = (train_labels - label_min) / (label_max - label_min)\n",
        "val_labels_normalized = (val_labels - label_min) / (label_max - label_min)\n",
        "\n",
        "\n",
        "# Crear datasets y DataLoaders\n",
        "train_dataset = BERTDataset(train_inputs, train_labels_normalized)\n",
        "val_dataset = BERTDataset(val_inputs, val_labels_normalized)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOjl4fSuIGJ3"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Cargar el modelo de BERT para clasificación de secuencias\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=1)\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1bF-D9v_ACl"
      },
      "outputs": [],
      "source": [
        "# Función para evaluar el modelo y almacenar predicciones\n",
        "def evaluate_model_and_store(model, val_loader):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    predictions = []\n",
        "    predictions_desnormalized = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "            logits = outputs.logits.squeeze().cpu().numpy()\n",
        "            labels = labels.cpu().numpy()\n",
        "            # Desnormalizar las predicciones\n",
        "            desnormalized_logits = logits * (label_max - label_min) + label_min\n",
        "\n",
        "            predictions.extend(logits)\n",
        "            true_labels.extend(labels)\n",
        "            predictions_desnormalized.extend(desnormalized_logits)\n",
        "\n",
        "    return predictions, true_labels, predictions_desnormalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1NNiBf9hCwM",
        "outputId": "96cb4534-6752-4bbd-fab5-784e85e8974a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 2.5376\n",
            "Epoch 2/10, Loss: 6.2035\n",
            "Epoch 3/10, Loss: 2.4982\n",
            "Epoch 4/10, Loss: 1.0203\n",
            "Epoch 5/10, Loss: 0.4518\n",
            "Epoch 6/10, Loss: 0.1713\n",
            "Epoch 7/10, Loss: 0.0671\n",
            "Epoch 8/10, Loss: 0.0249\n",
            "Epoch 9/10, Loss: 0.0081\n",
            "Epoch 10/10, Loss: 0.0042\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW  # Usar la implementación de PyTorch\n",
        "from transformers import get_scheduler\n",
        "\n",
        "epochs= 10\n",
        "\n",
        "# Configurar optimizador\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)  # Agregar weight decay\n",
        "num_training_steps = len(train_loader) * epochs\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "\n",
        "# Inicializar listas para almacenar resultados\n",
        "results = []\n",
        "predictions_list = []\n",
        "\n",
        "# Ciclo de entrenamiento modificado para almacenar resultados\n",
        "for epoch in range(epochs):  # Ajusta el número de épocas según sea necesario\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids= token_type_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Evaluar el modelo después de cada época\n",
        "    epoch_predictions, epoch_labels, epoch_predictions_desnormalized = evaluate_model_and_store(model, val_loader)\n",
        "    results.append({'Epoch': epoch + 1, 'Loss': total_loss})\n",
        "    predictions_list.extend(zip(epoch_labels, epoch_predictions, epoch_predictions_desnormalized))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrjc77kWauXB",
        "outputId": "3f06afa7-f43b-4dd3-d348-b2dd7c65f04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      True Label Normalized  Predicted Normalized  Predicted Desnormalized\n",
            "0                  0.428571              0.858262                 6.007833\n",
            "1                  0.714286              0.644851                 4.513955\n",
            "2                  0.571429              0.500728                 3.505096\n",
            "3                  0.000000              0.556332                 3.894327\n",
            "4                  1.000000              0.650930                 4.556510\n",
            "...                     ...                   ...                      ...\n",
            "5235               1.000000              0.558807                 3.911651\n",
            "5236               0.000000              0.425803                 2.980623\n",
            "5237               0.000000              0.719115                 5.033805\n",
            "5238               0.285714              0.430026                 3.010182\n",
            "5239               1.000000              0.880983                 6.166881\n",
            "\n",
            "[5240 rows x 3 columns]\n",
            "Resultados guardados en 'training_metrics.xlsx' y 'predictions.xlsx'\n"
          ]
        }
      ],
      "source": [
        "# Guardar métricas en Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel('/workspace/training_metrics.xlsx', index=False)\n",
        "\n",
        "# Guardar predicciones en Excel\n",
        "predictions_df = pd.DataFrame(predictions_list, columns=['True Label Normalized', 'Predicted Normalized', 'Predicted Desnormalized'])\n",
        "print(predictions_df)\n",
        "# Supongamos que estos son los valores mínimo y máximo originales de las etiquetas\n",
        "label_min = 0  # Reemplaza con el valor real\n",
        "label_max = 7  # Reemplaza con el valor real\n",
        "\n",
        "# Agregar la columna de True Label Original al dataframe\n",
        "predictions_df['True Label Original'] = predictions_df['True Label Normalized'] * (label_max - label_min) + label_min\n",
        "\n",
        "# Guardar los resultados en un nuevo archivo Excel\n",
        "predictions_df.to_excel('/workspace/predictions_with_true_labels.xlsx', index=False)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Resultados guardados en 'training_metrics.xlsx' y 'predictions.xlsx'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNkmlnqX_ACm"
      },
      "outputs": [],
      "source": [
        "# Guardar el modelo entrenado\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model.save_pretrained(\"bert_trained_model\")\n",
        "tokenizer.save_pretrained(\"bert_trained_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmuqPJhl_ACn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}